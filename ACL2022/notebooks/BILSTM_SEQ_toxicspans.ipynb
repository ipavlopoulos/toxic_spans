{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BILSTM_SEQ.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install ToxicSpans"
      ],
      "metadata": {
        "id": "2drz-ui79GPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ipavlopoulos/toxic_spans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzAIp0uY9N08",
        "outputId": "a682beb7-eb9d-4e38-c5e7-1d648e10fb4a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'toxic_spans'...\n",
            "remote: Enumerating objects: 437, done.\u001b[K\n",
            "remote: Counting objects: 100% (173/173), done.\u001b[K\n",
            "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
            "remote: Total 437 (delta 63), reused 138 (delta 44), pack-reused 264\u001b[K\n",
            "Receiving objects: 100% (437/437), 5.37 MiB | 21.82 MiB/s, done.\n",
            "Resolving deltas: 100% (190/190), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Requirements"
      ],
      "metadata": {
        "id": "LeDBvJ6Q8_-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt"
      ],
      "metadata": {
        "id": "FFnFPMt89DLt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4f3f54a-9f14-4e20-ba33-256a2a241d61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl (665.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 665.5 MB 20 kB/s \n",
            "\u001b[?25hCollecting tensorflow-text==2.7.3\n",
            "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting transformers==4.12.4\n",
            "  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 67.2 MB/s \n",
            "\u001b[?25hCollecting numpy==1.21.4\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 30.7 MB/s \n",
            "\u001b[?25hCollecting pandas==1.3.4\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 26.5 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==1.0.1\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting lime==0.2.0.1\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[K     |████████████████████████████████| 275 kB 61.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy==1.7.3 in /usr/local/lib/python3.7/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 8)) (1.7.3)\n",
            "Collecting tensorflow-gpu==2.7.0\n",
            "  Downloading tensorflow_gpu-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 489.6 MB 23 kB/s \n",
            "\u001b[?25hCollecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.2.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 78.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.37.1)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 72.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.47.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.14.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (14.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.7.3->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 80.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2022.6.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 40.3 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 54.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 5)) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==1.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.7/dist-packages (from lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (0.18.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (2.4.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (7.1.2)\n",
            "Building wheels for collected packages: lime, sacremoses\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283857 sha256=020b8006c06577ae4cba8564ee1f0b5b8c7c998d903f83a601ae6cec8c04a49b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/cb/e5/ac701e12d365a08917bf4c6171c0961bc880a8181359c66aa7\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=d2bbc269f3dfcd0c1021cccd4171ab5b8c207acca6761c64fb385ff571c4e969\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built lime sacremoses\n",
            "Installing collected packages: numpy, tensorflow-estimator, pyyaml, keras, gast, tokenizers, tensorflow, scikit-learn, sacremoses, huggingface-hub, transformers, tensorflow-text, tensorflow-gpu, pandas, lime\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.4.0 huggingface-hub-0.8.1 keras-2.7.0 lime-0.2.0.1 numpy-1.21.4 pandas-1.3.4 pyyaml-6.0 sacremoses-0.0.53 scikit-learn-1.0.1 tensorflow-2.7.0+zzzcolab20220506150900 tensorflow-estimator-2.7.0 tensorflow-gpu-2.7.0 tensorflow-text-2.7.3 tokenizers-0.10.3 transformers-4.12.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "7Qo4_eCS89kO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tCbD2MfA8d4F"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Metrics"
      ],
      "metadata": {
        "id": "deDZVOHU9cDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from toxic_spans.SemEval2021.evaluation import semeval2021\n",
        "from toxic_spans.SemEval2021.baselines import models\n",
        "\n",
        "def precision(predictions, gold):\n",
        "  if len(gold) == 0:\n",
        "    return 1. if len(predictions) == 0 else 0.\n",
        "  if len(predictions) == 0:\n",
        "    return 0.\n",
        "  predictions_set = set(predictions)\n",
        "  gold_set = set(gold)\n",
        "  nom = len(predictions_set.intersection(gold_set))\n",
        "  denom = len(predictions_set)\n",
        "  return float(nom)/float(denom)\n",
        "\n",
        "def recall(predictions, gold):\n",
        "  if len(gold) == 0:\n",
        "    return 1. if len(predictions) == 0 else 0.\n",
        "  if len(predictions) == 0:\n",
        "    return 0.\n",
        "  predictions_set = set(predictions)\n",
        "  gold_set = set(gold)\n",
        "  nom = len(predictions_set.intersection(gold_set))\n",
        "  denom = len(gold_set)\n",
        "  return float(nom)/float(denom)"
      ],
      "metadata": {
        "id": "PGt9dmJq9T5F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method for Preparing the dataset (literal_eval some columns)"
      ],
      "metadata": {
        "id": "FbWUFiMm9tSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset):\n",
        "  dataset.probability = dataset.probability.apply(literal_eval)\n",
        "  dataset.position = dataset.position.apply(literal_eval)\n",
        "  dataset.text = dataset.text.apply(literal_eval)\n",
        "  dataset['type'] = dataset['type'].apply(literal_eval)\n",
        "  dataset.position_probability = dataset.position_probability.apply(literal_eval)\n",
        "  if 'position_lbl'in dataset.columns:\n",
        "    dataset.position_lbl = dataset.position_lbl.apply(literal_eval)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "S1EpflVB9fww"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Align tokens with token labels"
      ],
      "metadata": {
        "id": "eVriqT8D-bD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for each token extract the probabilistic label \n",
        "def extract_xy(data, tokenizer = None):\n",
        "  X = []\n",
        "  y = []\n",
        "  t_of = []\n",
        "  for i in tqdm(range(data.shape[0])):\n",
        "    toks = []\n",
        "    labels = []\n",
        "    offsets = []\n",
        "    (tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(data.iloc[i].text_of_post)\n",
        "    for j in  range(len(tokens)):\n",
        "      span = []\n",
        "      token = data.iloc[i].text_of_post[start_offsets[j]: end_offsets[j]]\n",
        "      token_offset = [i for i in range(start_offsets[j], end_offsets[j])]\n",
        "      for char_off in token_offset:\n",
        "        if char_off in data.iloc[i].position_probability.keys(): # if in a span\n",
        "          span.append(data.position_probability.iloc[i][char_off])\n",
        "        else: #char not in a span\n",
        "          span.append(0)\n",
        "      labels.append(np.mean(span)) #this token has toxicity = with the mean of its chars\n",
        "      toks.append(token)\n",
        "      offsets.append([i for i in range(start_offsets[j], end_offsets[j])])\n",
        "    y.append(labels)\n",
        "    X.append(toks)\n",
        "    t_of.append(offsets)\n",
        "  return X,y,t_of\n",
        "\n",
        "from toxic_spans.ACL2022.models.seq import *\n",
        "\n",
        "data = pd.read_csv(\"toxic_spans/ACL2022/data/toxic_spans.csv\")\n",
        "data = prepare_dataset(data)\n",
        "\n",
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "x, y,t = extract_xy(data, tokenizer)\n",
        "\n",
        "data['tokens'], data['token_labels'], data['token_offsets'] = x, y, t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rtKQZqBYzL9",
        "outputId": "335c27bd-4184-48d9-bb64-3a14f42ae6e2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11006/11006 [33:41<00:00,  5.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train BILSTM_SEQ on a Random Train/dev/Test split"
      ],
      "metadata": {
        "id": "VyoamvBVPRMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "\n",
        "train, dev = train_test_split(data, test_size = 0.2, random_state = 0)\n",
        "dev, test = train_test_split(dev, test_size = 0.5, random_state = 0)\n",
        "train, dev, test = train.reset_index(), dev.reset_index(), test.reset_index()\n",
        "\n",
        "model = BILSTM_SEQ(patience = 5)\n",
        "hs = model.fit(train.tokens, train.token_labels, dev.tokens, dev.token_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr7DxzT6-oF7",
        "outputId": "0a38da09-e27e-4dc8-ea49-efc479b2dfd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size:  31873\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inputs (InputLayer)         [(None, 128)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 128, 200)          6374600   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128, 200)          0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 128, 256)         336896    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " layer_normalization (LayerN  (None, 128, 256)         512       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 128, 1)           257       \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            " tf.compat.v1.squeeze (TFOpL  None                     0         \n",
            " ambda)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,712,265\n",
            "Trainable params: 6,712,265\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "69/69 [==============================] - 16s 73ms/step - loss: 0.0176 - MSE: 0.0176 - MAE: 0.0502 - MSLE: 0.0073 - val_loss: 0.0042 - val_MSE: 0.0042 - val_MAE: 0.0211 - val_MSLE: 0.0027\n",
            "Epoch 2/1000\n",
            "69/69 [==============================] - 2s 30ms/step - loss: 0.0037 - MSE: 0.0037 - MAE: 0.0212 - MSLE: 0.0024 - val_loss: 0.0039 - val_MSE: 0.0039 - val_MAE: 0.0192 - val_MSLE: 0.0025\n",
            "Epoch 3/1000\n",
            "69/69 [==============================] - 2s 29ms/step - loss: 0.0032 - MSE: 0.0032 - MAE: 0.0193 - MSLE: 0.0020 - val_loss: 0.0038 - val_MSE: 0.0038 - val_MAE: 0.0206 - val_MSLE: 0.0025\n",
            "Epoch 4/1000\n",
            "69/69 [==============================] - 2s 29ms/step - loss: 0.0028 - MSE: 0.0028 - MAE: 0.0187 - MSLE: 0.0018 - val_loss: 0.0039 - val_MSE: 0.0039 - val_MAE: 0.0206 - val_MSLE: 0.0025\n",
            "Epoch 5/1000\n",
            "69/69 [==============================] - 2s 29ms/step - loss: 0.0027 - MSE: 0.0027 - MAE: 0.0184 - MSLE: 0.0016 - val_loss: 0.0041 - val_MSE: 0.0041 - val_MAE: 0.0219 - val_MSLE: 0.0026\n",
            "Epoch 6/1000\n",
            "69/69 [==============================] - 2s 29ms/step - loss: 0.0025 - MSE: 0.0025 - MAE: 0.0179 - MSLE: 0.0015 - val_loss: 0.0040 - val_MSE: 0.0040 - val_MAE: 0.0224 - val_MSLE: 0.0027\n",
            "Epoch 7/1000\n",
            "69/69 [==============================] - 2s 29ms/step - loss: 0.0023 - MSE: 0.0023 - MAE: 0.0168 - MSLE: 0.0014 - val_loss: 0.0042 - val_MSE: 0.0042 - val_MAE: 0.0231 - val_MSLE: 0.0027\n",
            "Epoch 8/1000\n",
            "69/69 [==============================] - ETA: 0s - loss: 0.0020 - MSE: 0.0020 - MAE: 0.0158 - MSLE: 0.0012Restoring model weights from the end of the best epoch: 3.\n",
            "69/69 [==============================] - 2s 29ms/step - loss: 0.0020 - MSE: 0.0020 - MAE: 0.0158 - MSLE: 0.0012 - val_loss: 0.0042 - val_MSE: 0.0042 - val_MAE: 0.0236 - val_MSLE: 0.0028\n",
            "Epoch 00008: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "SC4LCm1yi-IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_offsets = model.get_toxic_offsets(test.tokens, 0.5)\n",
        "pred_char_offsets = model.get_toxic_char_offsets(test.token_offsets, pred_offsets)\n",
        "\n",
        "f1 = np.mean([semeval2021.f1(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "pr = np.mean([precision(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "rec = np.mean([recall(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "\n",
        "print(\"F1: \",f1)\n",
        "print(\"Recall: \",rec)\n",
        "print(\"Precision: \",pr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANWUOwPsO712",
        "outputId": "6b16c78f-8e20-48a3-f62f-70d7f45f4498"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1:  0.5952969505512168\n",
            "Recall:  0.5975387932212997\n",
            "Precision:  0.6033353567413513\n"
          ]
        }
      ]
    }
  ]
}