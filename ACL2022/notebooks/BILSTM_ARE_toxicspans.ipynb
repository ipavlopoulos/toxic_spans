{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install ToxicSpans"
      ],
      "metadata": {
        "id": "2drz-ui79GPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ipavlopoulos/toxic_spans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzAIp0uY9N08",
        "outputId": "46a74144-6419-412f-e977-4cee3f64398b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'toxic_spans'...\n",
            "remote: Enumerating objects: 469, done.\u001b[K\n",
            "remote: Counting objects: 100% (201/201), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 469 (delta 80), reused 142 (delta 45), pack-reused 268\u001b[K\n",
            "Receiving objects: 100% (469/469), 5.39 MiB | 9.47 MiB/s, done.\n",
            "Resolving deltas: 100% (210/210), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Requirements"
      ],
      "metadata": {
        "id": "LeDBvJ6Q8_-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt"
      ],
      "metadata": {
        "id": "FFnFPMt89DLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7c724e-1a24-43c3-ba97-696a24b6bc46"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.7.0 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-text==2.7.3 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 2)) (2.7.3)\n",
            "Requirement already satisfied: transformers==4.12.4 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (4.12.4)\n",
            "Requirement already satisfied: numpy==1.21.4 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 4)) (1.21.4)\n",
            "Requirement already satisfied: pandas==1.3.4 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 5)) (1.3.4)\n",
            "Requirement already satisfied: scikit-learn==1.0.1 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: lime==0.2.0.1 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (0.2.0.1)\n",
            "Requirement already satisfied: scipy==1.7.3 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 8)) (1.7.3)\n",
            "Requirement already satisfied: tensorflow-gpu==2.7.0 in /usr/local/lib/python3.8/dist-packages (from -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 9)) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.0.7)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (15.0.6.1)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.38.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.19.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.51.3)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text==2.7.3->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 2)) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (3.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (4.64.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (0.0.53)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (0.10.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 5)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.3.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn==1.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (3.5.3)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.8/dist-packages (from lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (3.0)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (8.4.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (2023.2.27)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (2.9.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (57.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.16.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (4.0.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (4.38.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (8.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "7Qo4_eCS89kO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tCbD2MfA8d4F"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Metrics"
      ],
      "metadata": {
        "id": "deDZVOHU9cDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from toxic_spans.SemEval2021.evaluation import semeval2021\n",
        "from toxic_spans.SemEval2021.baselines import models\n",
        "\n",
        "def precision(predictions, gold):\n",
        "  if len(gold) == 0:\n",
        "    return 1. if len(predictions) == 0 else 0.\n",
        "  if len(predictions) == 0:\n",
        "    return 0.\n",
        "  predictions_set = set(predictions)\n",
        "  gold_set = set(gold)\n",
        "  nom = len(predictions_set.intersection(gold_set))\n",
        "  denom = len(predictions_set)\n",
        "  return float(nom)/float(denom)\n",
        "\n",
        "def recall(predictions, gold):\n",
        "  if len(gold) == 0:\n",
        "    return 1. if len(predictions) == 0 else 0.\n",
        "  if len(predictions) == 0:\n",
        "    return 0.\n",
        "  predictions_set = set(predictions)\n",
        "  gold_set = set(gold)\n",
        "  nom = len(predictions_set.intersection(gold_set))\n",
        "  denom = len(gold_set)\n",
        "  return float(nom)/float(denom)"
      ],
      "metadata": {
        "id": "PGt9dmJq9T5F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method for Preparing the dataset (literal_eval some columns)"
      ],
      "metadata": {
        "id": "FbWUFiMm9tSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset):\n",
        "  dataset.probability = dataset.probability.apply(literal_eval)\n",
        "  dataset.position = dataset.position.apply(literal_eval)\n",
        "  dataset.text = dataset.text.apply(literal_eval)\n",
        "  dataset['type'] = dataset['type'].apply(literal_eval)\n",
        "  dataset.position_probability = dataset.position_probability.apply(literal_eval)\n",
        "  if 'position_lbl'in dataset.columns:\n",
        "    dataset.position_lbl = dataset.position_lbl.apply(literal_eval)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "S1EpflVB9fww"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Align tokens with token labels"
      ],
      "metadata": {
        "id": "eVriqT8D-bD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for each token extract the probabilistic label \n",
        "def extract_xy(data, tokenizer = None):\n",
        "  X = []\n",
        "  y = []\n",
        "  t_of = []\n",
        "  for i in tqdm(range(data.shape[0])):\n",
        "    toks = []\n",
        "    labels = []\n",
        "    offsets = []\n",
        "    (tokens, start_offsets, end_offsets) = tokenizer.tokenize_with_offsets(data.iloc[i].text_of_post)\n",
        "    for j in  range(len(tokens)):\n",
        "      span = []\n",
        "      token = data.iloc[i].text_of_post[start_offsets[j]: end_offsets[j]]\n",
        "      token_offset = [i for i in range(start_offsets[j], end_offsets[j])]\n",
        "      for char_off in token_offset:\n",
        "        if char_off in data.iloc[i].position_probability.keys(): # if in a span\n",
        "          span.append(data.position_probability.iloc[i][char_off])\n",
        "        else: #char not in a span\n",
        "          span.append(0)\n",
        "      labels.append(np.mean(span)) #this token has toxicity = with the mean of its chars\n",
        "      toks.append(token)\n",
        "      offsets.append([i for i in range(start_offsets[j], end_offsets[j])])\n",
        "    y.append(labels)\n",
        "    X.append(toks)\n",
        "    t_of.append(offsets)\n",
        "  return X,y,t_of\n",
        "\n",
        "from toxic_spans.ACL2022.models.are import *\n",
        "\n",
        "data = pd.read_csv(\"toxic_spans/ACL2022/data/toxic_spans.csv\")\n",
        "data = prepare_dataset(data)\n",
        "\n",
        "tokenizer = tf_text.UnicodeScriptTokenizer()\n",
        "x, y,t = extract_xy(data, tokenizer)\n",
        "\n",
        "data['tokens'], data['token_labels'], data['token_offsets'] = x, y, t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rtKQZqBYzL9",
        "outputId": "6ffbfe86-4e06-434f-ef97-2818ba67ebef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11006/11006 [40:40<00:00,  4.51it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Download data for augmentation "
      ],
      "metadata": {
        "id": "LbQbGcptPwCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1ApFrfl3UDaAbYJ4GhuZIhLGUGHZUxiPH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjF_gY7OPy3R",
        "outputId": "ec8f7d72-8893-449a-8300-da9c7e76ded1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ApFrfl3UDaAbYJ4GhuZIhLGUGHZUxiPH\n",
            "To: /content/5k_augmentation.csv\n",
            "100% 10.6M/10.6M [00:00<00:00, 28.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Download data for roc auc evaluation "
      ],
      "metadata": {
        "id": "0Fu07GFPP2yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1qN2s3d2qTNp4JuO_7GTjLmltWCatuUrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b5LGZ-OP6uh",
        "outputId": "80dc58dc-f721-418c-ecd9-427cf2f4748b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:127: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qN2s3d2qTNp4JuO_7GTjLmltWCatuUrt\n",
            "To: /content/for_auc_eval.csv\n",
            "100% 5.81M/5.81M [00:00<00:00, 24.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train BILSTM_ARE on a Random Train/dev/Test split"
      ],
      "metadata": {
        "id": "VyoamvBVPRMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "data['toxicity'] = [1 for i in range(data.shape[0])]\n",
        "\n",
        "#compute dummy labels for the attention loss \n",
        "dummy_labels = []\n",
        "for i in tqdm(range(data.shape[0])):\n",
        "  instance = []\n",
        "  for j,token in enumerate(data.iloc[i].tokens):\n",
        "    instance.append(0)\n",
        "  dummy_labels.append(instance)\n",
        "data['dummy_labels'] = dummy_labels\n",
        "\n",
        "#prepare data for augmentation \n",
        "augmentation = pd.read_csv(\"5k_augmentation.csv\")\n",
        "augmentation.tokens = augmentation.tokens.apply(literal_eval)\n",
        "augmentation.token_offsets = augmentation.token_offsets.apply(literal_eval)\n",
        "augmentation.dummy_labels = augmentation.dummy_labels.apply(literal_eval)\n",
        "augmentation.toxicity = augmentation.toxicity.apply(lambda x: 1 if x > 0.5 else 0)\n",
        "\n",
        "#prepare dataset for roc auc eval \n",
        "auc_eval = pd.read_csv(\"for_auc_eval.csv\") \n",
        "auc_eval.tokens = auc_eval.tokens.apply(literal_eval)\n",
        "auc_eval.toxicity = auc_eval.toxicity.apply(lambda x: 1 if x > 0.5 else 0)\n",
        "\n",
        "\n",
        "train, dev = train_test_split(data, test_size = 0.2, random_state = 0)\n",
        "dev, test = train_test_split(dev, test_size = 0.5, random_state = 0)\n",
        "train, dev, test = train.reset_index(), dev.reset_index(), test.reset_index()\n",
        "\n",
        "\n",
        "model = BILSTM_ARE(patience = 5)\n",
        "\n",
        "#augment training set\n",
        "train = pd.concat([train, augmentation]).sample(frac = 1).reset_index()\n",
        "\n",
        "#train the model \n",
        "hs = model.fit(train.tokens, train.toxicity, train.dummy_labels, dev.tokens, dev.toxicity, dev.dummy_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr7DxzT6-oF7",
        "outputId": "777b43be-2f7d-40ba-83ea-e8c8f8fe2fd8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11006/11006 [00:01<00:00, 5761.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FFFFFF\n",
            "Vocab size:  47270\n",
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, 128, 200)     9454000     ['inputs[0][0]']                 \n",
            "                                                                                                  \n",
            " bidirectional_6 (Bidirectional  (None, 128, 256)    336896      ['embedding_6[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " time_distributed_12 (TimeDistr  (None, 128, 128)    32896       ['bidirectional_6[0][0]']        \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " time_distributed_13 (TimeDistr  (None, 128, 1)      129         ['time_distributed_12[0][0]']    \n",
            " ibuted)                                                                                          \n",
            "                                                                                                  \n",
            " tf.nn.softmax_6 (TFOpLambda)   (None, 128, 1)       0           ['time_distributed_13[0][0]']    \n",
            "                                                                                                  \n",
            " tf.math.multiply_6 (TFOpLambda  (None, 128, 256)    0           ['tf.nn.softmax_6[0][0]',        \n",
            " )                                                                'bidirectional_6[0][0]']        \n",
            "                                                                                                  \n",
            " tf.math.reduce_sum_6 (TFOpLamb  (None, 256)         0           ['tf.math.multiply_6[0][0]']     \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " classification (Dense)         (None, 1)            257         ['tf.math.reduce_sum_6[0][0]']   \n",
            "                                                                                                  \n",
            " tf.compat.v1.squeeze_6 (TFOpLa  None                0           ['tf.nn.softmax_6[0][0]']        \n",
            " mbda)                                                                                            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9,824,178\n",
            "Trainable params: 9,824,178\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/200\n",
            "108/108 [==============================] - 15s 59ms/step - loss: 0.2053 - classification_loss: 0.2053 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.2053 - classification_MAE: 0.3444 - classification_MSLE: 0.0941 - tf.compat.v1.squeeze_6_MSE: 0.0012 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 8.3722e-04 - val_loss: 0.0495 - val_classification_loss: 0.0495 - val_tf.compat.v1.squeeze_6_loss: 0.0000e+00 - val_classification_MSE: 0.0495 - val_classification_MAE: 0.1553 - val_classification_MSLE: 0.0158 - val_tf.compat.v1.squeeze_6_MSE: 0.0045 - val_tf.compat.v1.squeeze_6_MAE: 0.0078 - val_tf.compat.v1.squeeze_6_MSLE: 0.0025\n",
            "Epoch 2/200\n",
            "108/108 [==============================] - 3s 29ms/step - loss: 0.0686 - classification_loss: 0.0686 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.0686 - classification_MAE: 0.1837 - classification_MSLE: 0.0327 - tf.compat.v1.squeeze_6_MSE: 0.0044 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 0.0024 - val_loss: 0.0457 - val_classification_loss: 0.0457 - val_tf.compat.v1.squeeze_6_loss: 0.0000e+00 - val_classification_MSE: 0.0457 - val_classification_MAE: 0.1409 - val_classification_MSLE: 0.0154 - val_tf.compat.v1.squeeze_6_MSE: 0.0056 - val_tf.compat.v1.squeeze_6_MAE: 0.0078 - val_tf.compat.v1.squeeze_6_MSLE: 0.0029\n",
            "Epoch 3/200\n",
            "108/108 [==============================] - 3s 28ms/step - loss: 0.0323 - classification_loss: 0.0323 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.0323 - classification_MAE: 0.1134 - classification_MSLE: 0.0150 - tf.compat.v1.squeeze_6_MSE: 0.0050 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 0.0026 - val_loss: 0.0374 - val_classification_loss: 0.0374 - val_tf.compat.v1.squeeze_6_loss: 0.0000e+00 - val_classification_MSE: 0.0374 - val_classification_MAE: 0.1148 - val_classification_MSLE: 0.0132 - val_tf.compat.v1.squeeze_6_MSE: 0.0059 - val_tf.compat.v1.squeeze_6_MAE: 0.0078 - val_tf.compat.v1.squeeze_6_MSLE: 0.0031\n",
            "Epoch 4/200\n",
            "108/108 [==============================] - 3s 28ms/step - loss: 0.0122 - classification_loss: 0.0122 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.0122 - classification_MAE: 0.0617 - classification_MSLE: 0.0057 - tf.compat.v1.squeeze_6_MSE: 0.0053 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 0.0027 - val_loss: 0.0404 - val_classification_loss: 0.0404 - val_tf.compat.v1.squeeze_6_loss: 0.0000e+00 - val_classification_MSE: 0.0404 - val_classification_MAE: 0.0976 - val_classification_MSLE: 0.0153 - val_tf.compat.v1.squeeze_6_MSE: 0.0060 - val_tf.compat.v1.squeeze_6_MAE: 0.0078 - val_tf.compat.v1.squeeze_6_MSLE: 0.0031\n",
            "Epoch 5/200\n",
            "108/108 [==============================] - 3s 29ms/step - loss: 0.0064 - classification_loss: 0.0064 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.0064 - classification_MAE: 0.0419 - classification_MSLE: 0.0030 - tf.compat.v1.squeeze_6_MSE: 0.0054 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 0.0028 - val_loss: 0.0647 - val_classification_loss: 0.0647 - val_tf.compat.v1.squeeze_6_loss: 0.0000e+00 - val_classification_MSE: 0.0647 - val_classification_MAE: 0.1217 - val_classification_MSLE: 0.0258 - val_tf.compat.v1.squeeze_6_MSE: 0.0059 - val_tf.compat.v1.squeeze_6_MAE: 0.0078 - val_tf.compat.v1.squeeze_6_MSLE: 0.0030\n",
            "Epoch 6/200\n",
            "108/108 [==============================] - 3s 28ms/step - loss: 0.0050 - classification_loss: 0.0050 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.0050 - classification_MAE: 0.0359 - classification_MSLE: 0.0023 - tf.compat.v1.squeeze_6_MSE: 0.0055 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 0.0028 - val_loss: 0.0440 - val_classification_loss: 0.0440 - val_tf.compat.v1.squeeze_6_loss: 0.0000e+00 - val_classification_MSE: 0.0440 - val_classification_MAE: 0.0939 - val_classification_MSLE: 0.0172 - val_tf.compat.v1.squeeze_6_MSE: 0.0062 - val_tf.compat.v1.squeeze_6_MAE: 0.0078 - val_tf.compat.v1.squeeze_6_MSLE: 0.0032\n",
            "Epoch 7/200\n",
            "108/108 [==============================] - 3s 28ms/step - loss: 0.0036 - classification_loss: 0.0036 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.0036 - classification_MAE: 0.0294 - classification_MSLE: 0.0017 - tf.compat.v1.squeeze_6_MSE: 0.0056 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 0.0028 - val_loss: 0.0516 - val_classification_loss: 0.0516 - val_tf.compat.v1.squeeze_6_loss: 0.0000e+00 - val_classification_MSE: 0.0516 - val_classification_MAE: 0.1006 - val_classification_MSLE: 0.0204 - val_tf.compat.v1.squeeze_6_MSE: 0.0061 - val_tf.compat.v1.squeeze_6_MAE: 0.0078 - val_tf.compat.v1.squeeze_6_MSLE: 0.0031\n",
            "Epoch 8/200\n",
            "107/108 [============================>.] - ETA: 0s - loss: 0.0024 - classification_loss: 0.0024 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.0024 - classification_MAE: 0.0242 - classification_MSLE: 0.0011 - tf.compat.v1.squeeze_6_MSE: 0.0056 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 0.0028Restoring model weights from the end of the best epoch: 3.\n",
            "108/108 [==============================] - 3s 28ms/step - loss: 0.0024 - classification_loss: 0.0024 - tf.compat.v1.squeeze_6_loss: 0.0000e+00 - classification_MSE: 0.0024 - classification_MAE: 0.0242 - classification_MSLE: 0.0011 - tf.compat.v1.squeeze_6_MSE: 0.0056 - tf.compat.v1.squeeze_6_MAE: 0.0078 - tf.compat.v1.squeeze_6_MSLE: 0.0028 - val_loss: 0.0397 - val_classification_loss: 0.0397 - val_tf.compat.v1.squeeze_6_loss: 0.0000e+00 - val_classification_MSE: 0.0397 - val_classification_MAE: 0.0835 - val_classification_MSLE: 0.0155 - val_tf.compat.v1.squeeze_6_MSE: 0.0062 - val_tf.compat.v1.squeeze_6_MAE: 0.0078 - val_tf.compat.v1.squeeze_6_MSLE: 0.0032\n",
            "Epoch 00008: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "SC4LCm1yi-IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_th = model.finetune_att_threshold(dev.tokens, dev.token_offsets, dev.position)\n",
        "pred_offsets = model.get_toxic_offsets(test.tokens, threshold=opt_th)\n",
        "pred_char_offsets = model.get_toxic_char_offsets(test.token_offsets, pred_offsets)\n",
        "\n",
        "f1 = np.mean([semeval2021.f1(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "pr = np.mean([precision(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "rec = np.mean([recall(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "\n",
        "preds, _ = model.predict(auc_eval.tokens)\n",
        "auc = roc_auc_score(auc_eval.toxicity, preds)\n",
        "\n",
        "print(\"F1: \",f1)\n",
        "print(\"Recall: \",rec)\n",
        "print(\"Precision: \" ,pr)\n",
        "print(\"ROC AUC: \",auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANWUOwPsO712",
        "outputId": "ecb3867f-06cd-4c60-ba0a-3e97020018e9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal threshold is:  0.95  with F1 score =  0.5691043343672841\n",
            "F1:  0.558043992473185\n",
            "Recall:  0.5524877592153589\n",
            "Precision:  0.5731153496821072\n",
            "ROC AUC:  0.9012764444444444\n"
          ]
        }
      ]
    }
  ]
}