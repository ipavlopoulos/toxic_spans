{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install ToxicSpans"
      ],
      "metadata": {
        "id": "2drz-ui79GPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ipavlopoulos/toxic_spans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzAIp0uY9N08",
        "outputId": "995ab39f-0b11-4244-d76c-9b648939e8e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'toxic_spans'...\n",
            "remote: Enumerating objects: 483, done.\u001b[K\n",
            "remote: Counting objects: 100% (215/215), done.\u001b[K\n",
            "remote: Compressing objects: 100% (163/163), done.\u001b[K\n",
            "remote: Total 483 (delta 89), reused 142 (delta 45), pack-reused 268\u001b[K\n",
            "Receiving objects: 100% (483/483), 5.40 MiB | 17.84 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Requirements"
      ],
      "metadata": {
        "id": "LeDBvJ6Q8_-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt"
      ],
      "metadata": {
        "id": "FFnFPMt89DLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "811f8ef5-255e-4d31-c523-509657664eca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading tensorflow-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.7/489.7 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text==2.7.3\n",
            "  Downloading tensorflow_text-2.7.3-cp39-cp39-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.12.4\n",
            "  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.21.4\n",
            "  Downloading numpy-1.21.4-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.3.4\n",
            "  Downloading pandas-1.3.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.0.1\n",
            "  Downloading scikit_learn-1.0.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lime==0.2.0.1\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 KB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scipy==1.7.3\n",
            "  Downloading scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.8/39.8 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-gpu==2.7.0\n",
            "  Downloading tensorflow_gpu-2.7.0-cp39-cp39-manylinux2010_x86_64.whl (489.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.7/489.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.8.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.1/463.1 KB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.31.0)\n",
            "Collecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.51.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (15.0.6.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.40.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.4.0)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.4.0)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.11.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-text==2.7.3->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 2)) (0.13.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (3.10.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2022.10.31)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.3.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 5)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.3.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 6)) (1.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (3.7.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.9/dist-packages (from lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (3.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (8.4.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (2023.3.21)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.12->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (2.25.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.4.3)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.16.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (67.6.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (1.0.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (4.39.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from sacremoses->transformers==4.12.4->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 3)) (8.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->lime==0.2.0.1->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 7)) (3.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (6.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0->-r toxic_spans/ACL2022/requirements_for_toxic_spans_exps.txt (line 1)) (3.2.2)\n",
            "Building wheels for collected packages: lime, sacremoses\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283859 sha256=c09aa426cad2afe6fb8cd29b893c080647b96368472ae29e5a30c25b37423576\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/d7/c9/5a0130d06d6310bc6cbe55220e6e72dcb8c4eff9a478717066\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895259 sha256=0e3dbdd19acc4a7c2e8750fee5688ac5c3c620283efaa749e942e8914ba1320e\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built lime sacremoses\n",
            "Installing collected packages: tokenizers, tensorflow-estimator, keras, flatbuffers, sacremoses, numpy, scipy, pandas, keras-preprocessing, huggingface-hub, transformers, scikit-learn, lime, tensorflow-gpu, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.11.0\n",
            "    Uninstalling tensorflow-estimator-2.11.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.11.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.11.0\n",
            "    Uninstalling keras-2.11.0:\n",
            "      Successfully uninstalled keras-2.11.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 23.3.3\n",
            "    Uninstalling flatbuffers-23.3.3:\n",
            "      Successfully uninstalled flatbuffers-23.3.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.4\n",
            "    Uninstalling pandas-1.4.4:\n",
            "      Successfully uninstalled pandas-1.4.4\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.11.0\n",
            "    Uninstalling tensorflow-2.11.0:\n",
            "      Successfully uninstalled tensorflow-2.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.10.1 requires pandas>=1.3.5, but you have pandas 1.3.4 which is incompatible.\n",
            "mizani 0.8.1 requires pandas>=1.3.5, but you have pandas 1.3.4 which is incompatible.\n",
            "imbalanced-learn 0.10.1 requires scikit-learn>=1.0.2, but you have scikit-learn 1.0.1 which is incompatible.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flatbuffers-2.0.7 huggingface-hub-0.13.3 keras-2.7.0 keras-preprocessing-1.1.2 lime-0.2.0.1 numpy-1.21.4 pandas-1.3.4 sacremoses-0.0.53 scikit-learn-1.0.1 scipy-1.7.3 tensorflow-2.7.0 tensorflow-estimator-2.7.0 tensorflow-gpu-2.7.0 tensorflow-text-2.7.3 tokenizers-0.10.3 transformers-4.12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "7Qo4_eCS89kO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tCbD2MfA8d4F"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define Metrics"
      ],
      "metadata": {
        "id": "deDZVOHU9cDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from toxic_spans.SemEval2021.evaluation import semeval2021\n",
        "from toxic_spans.SemEval2021.baselines import models\n",
        "\n",
        "def precision(predictions, gold):\n",
        "  if len(gold) == 0:\n",
        "    return 1. if len(predictions) == 0 else 0.\n",
        "  if len(predictions) == 0:\n",
        "    return 0.\n",
        "  predictions_set = set(predictions)\n",
        "  gold_set = set(gold)\n",
        "  nom = len(predictions_set.intersection(gold_set))\n",
        "  denom = len(predictions_set)\n",
        "  return float(nom)/float(denom)\n",
        "\n",
        "def recall(predictions, gold):\n",
        "  if len(gold) == 0:\n",
        "    return 1. if len(predictions) == 0 else 0.\n",
        "  if len(predictions) == 0:\n",
        "    return 0.\n",
        "  predictions_set = set(predictions)\n",
        "  gold_set = set(gold)\n",
        "  nom = len(predictions_set.intersection(gold_set))\n",
        "  denom = len(gold_set)\n",
        "  return float(nom)/float(denom)"
      ],
      "metadata": {
        "id": "PGt9dmJq9T5F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method for Preparing the dataset (literal_eval some columns)"
      ],
      "metadata": {
        "id": "FbWUFiMm9tSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset):\n",
        "  dataset.probability = dataset.probability.apply(literal_eval)\n",
        "  dataset.position = dataset.position.apply(literal_eval)\n",
        "  dataset.text = dataset.text.apply(literal_eval)\n",
        "  dataset['type'] = dataset['type'].apply(literal_eval)\n",
        "  dataset.position_probability = dataset.position_probability.apply(literal_eval)\n",
        "  if 'position_lbl'in dataset.columns:\n",
        "    dataset.position_lbl = dataset.position_lbl.apply(literal_eval)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "S1EpflVB9fww"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Align tokens with token labels"
      ],
      "metadata": {
        "id": "eVriqT8D-bD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for each token extract the probabilistic label \n",
        "def extract_xy(data, tokenizer):\n",
        "    \"\"\"\n",
        "    This method aligns x and y according to BERT's sub-tokens\n",
        "    :param data: the dataframe (read toxic_spans.csv)\n",
        "    :param tokenizer: bert's tokenizer\n",
        "    :return: x and y aligned (subtokens aligned with subtokens toxicity labels)\n",
        "    \"\"\"\n",
        "    \n",
        "    x = [] #tokens (or subtokens)\n",
        "    y = [] #token labels\n",
        "    for i in tqdm(range(data.shape[0])):\n",
        "      subtokens = []\n",
        "      token_labels = []\n",
        "      tokenized_batch : BatchEncoding = tokenizer(data.iloc[i].text_of_post)\n",
        "      tokenized_text :Encoding = tokenized_batch[0]\n",
        "      tokens = ['[CLS]'] + tokenizer.tokenize(data.iloc[i].text_of_post) + ['[SEP]']\n",
        "      for j,token in enumerate(tokens):\n",
        "        if j == 0 or j == len(tokens) - 1: #ignore ['CLS'] and ['SEP'] tokens\n",
        "         continue\n",
        "        else:\n",
        "          (start, end) = tokenized_text.token_to_chars(j) #char offset of jth sub-token (in the original text)\n",
        "          span_score = []\n",
        "          for ch_offset in range(start,end):\n",
        "            if ch_offset in data.iloc[i].position_probability.keys():\n",
        "              span_score.append(data.iloc[i].position_probability[ch_offset])\n",
        "            else:\n",
        "              span_score.append(0)\n",
        "          token_labels.append(np.mean(span_score))\n",
        "          subtokens.append(token)\n",
        "      x.append(subtokens)\n",
        "      y.append(token_labels)\n",
        "    return x, y \n",
        "    \n",
        "from toxic_spans.ACL2022.models.are import *\n",
        "\n",
        "model = BERT_ARE(patience = 5)\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"toxic_spans/ACL2022/data/toxic_spans.csv\")\n",
        "data = prepare_dataset(data)\n",
        "\n",
        "tokenizer = model.tokenizer\n",
        "x, y = extract_xy(data, tokenizer)\n",
        "\n",
        "data['tokens'], data['token_labels'] = x, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rtKQZqBYzL9",
        "outputId": "c17220cf-21be-4278-dc43-b01a9d1b625e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "100%|██████████| 11006/11006 [03:49<00:00, 48.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Download data for augmentation "
      ],
      "metadata": {
        "id": "LbQbGcptPwCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1ApFrfl3UDaAbYJ4GhuZIhLGUGHZUxiPH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjF_gY7OPy3R",
        "outputId": "3cf4467c-5099-4fd1-b150-43cf26fb6237"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ApFrfl3UDaAbYJ4GhuZIhLGUGHZUxiPH\n",
            "To: /content/5k_augmentation.csv\n",
            "100% 10.6M/10.6M [00:00<00:00, 27.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Download data for roc auc evaluation "
      ],
      "metadata": {
        "id": "0Fu07GFPP2yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1qN2s3d2qTNp4JuO_7GTjLmltWCatuUrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b5LGZ-OP6uh",
        "outputId": "52d98d25-74a4-4f4d-f5d2-d8cca7a4263c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1qN2s3d2qTNp4JuO_7GTjLmltWCatuUrt\n",
            "To: /content/for_auc_eval.csv\n",
            "100% 5.81M/5.81M [00:00<00:00, 21.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train BERT_ARE on a Random Train/dev/Test split"
      ],
      "metadata": {
        "id": "VyoamvBVPRMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "\n",
        "\n",
        "data['toxicity'] = [1 for i in range(data.shape[0])]\n",
        "\n",
        "#prepare data for augmentation \n",
        "augmentation = pd.read_csv(\"5k_augmentation.csv\")\n",
        "augmentation.toxicity = augmentation.toxicity.apply(lambda x: 1 if x > 0.5 else 0)\n",
        "\n",
        "#prepare dataset for roc auc eval \n",
        "auc_eval = pd.read_csv(\"for_auc_eval.csv\") \n",
        "auc_eval.toxicity = auc_eval.toxicity.apply(lambda x: 1 if x > 0.5 else 0)\n",
        "\n",
        "\n",
        "train, dev = train_test_split(data, test_size = 0.2, random_state = 0)\n",
        "dev, test = train_test_split(dev, test_size = 0.5, random_state = 0)\n",
        "train, dev, test = train.reset_index(), dev.reset_index(), test.reset_index()\n",
        "\n",
        "\n",
        "\n",
        "#augment training set\n",
        "train = pd.concat([train, augmentation]).sample(frac = 1).reset_index()\n",
        "\n",
        "#train the model \n",
        "hs = model.fit(train.text_of_post, train.toxicity, dev.text_of_post, dev.toxicity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fr7DxzT6-oF7",
        "outputId": "544006a3-57e7-4e30-8fc2-81f0a4f9a122"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/13804 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "100%|██████████| 13804/13804 [00:03<00:00, 4302.66it/s]\n",
            "100%|██████████| 1101/1101 [00:00<00:00, 4664.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " input_masks (InputLayer)       [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " segment_ids (InputLayer)       [(None, 128)]        0           []                               \n",
            "                                                                                                  \n",
            " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  108310272   ['input_ids[0][0]',              \n",
            "                                thPoolingAndCrossAt               'input_masks[0][0]',            \n",
            "                                tentions(last_hidde               'segment_ids[0][0]']            \n",
            "                                n_state=(None, 128,                                               \n",
            "                                 768),                                                            \n",
            "                                 pooler_output=(Non                                               \n",
            "                                e, 768),                                                          \n",
            "                                 past_key_values=No                                               \n",
            "                                ne, hidden_states=N                                               \n",
            "                                one, attentions=((N                                               \n",
            "                                one, 12, None, 128)                                               \n",
            "                                , (None, 12, None,                                                \n",
            "                                128),                                                             \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28),                                                              \n",
            "                                 (None, 12, None, 1                                               \n",
            "                                28)),                                                             \n",
            "                                 cross_attentions=N                                               \n",
            "                                one)                                                              \n",
            "                                                                                                  \n",
            " tf.__operators__.getitem (Slic  (None, 768)         0           ['tf_bert_model_1[0][12]']       \n",
            " ingOpLambda)                                                                                     \n",
            "                                                                                                  \n",
            " classification (Dense)         (None, 1)            769         ['tf.__operators__.getitem[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 108,311,041\n",
            "Trainable params: 108,311,041\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "432/432 [==============================] - 358s 795ms/step - loss: 0.2275 - accuracy: 0.9060 - precision: 0.9252 - recall: 0.9631 - val_loss: 0.0868 - val_accuracy: 0.9718 - val_precision: 1.0000 - val_recall: 0.9718\n",
            "Epoch 2/100\n",
            "432/432 [==============================] - 348s 806ms/step - loss: 0.1049 - accuracy: 0.9607 - precision: 0.9720 - recall: 0.9802 - val_loss: 0.0343 - val_accuracy: 0.9873 - val_precision: 1.0000 - val_recall: 0.9873\n",
            "Epoch 3/100\n",
            "432/432 [==============================] - 348s 806ms/step - loss: 0.0489 - accuracy: 0.9840 - precision: 0.9893 - recall: 0.9912 - val_loss: 0.0626 - val_accuracy: 0.9791 - val_precision: 1.0000 - val_recall: 0.9791\n",
            "Epoch 4/100\n",
            "432/432 [==============================] - 348s 805ms/step - loss: 0.0239 - accuracy: 0.9920 - precision: 0.9942 - recall: 0.9961 - val_loss: 0.0524 - val_accuracy: 0.9855 - val_precision: 1.0000 - val_recall: 0.9855\n",
            "Epoch 5/100\n",
            "432/432 [==============================] - 348s 805ms/step - loss: 0.0156 - accuracy: 0.9953 - precision: 0.9967 - recall: 0.9975 - val_loss: 0.1078 - val_accuracy: 0.9682 - val_precision: 1.0000 - val_recall: 0.9682\n",
            "Epoch 6/100\n",
            "432/432 [==============================] - 348s 805ms/step - loss: 0.0078 - accuracy: 0.9980 - precision: 0.9988 - recall: 0.9988 - val_loss: 0.1885 - val_accuracy: 0.9637 - val_precision: 1.0000 - val_recall: 0.9637\n",
            "Epoch 7/100\n",
            "432/432 [==============================] - ETA: 0s - loss: 0.0110 - accuracy: 0.9963 - precision: 0.9974 - recall: 0.9981Restoring model weights from the end of the best epoch: 2.\n",
            "432/432 [==============================] - 348s 806ms/step - loss: 0.0110 - accuracy: 0.9963 - precision: 0.9974 - recall: 0.9981 - val_loss: 0.1014 - val_accuracy: 0.9800 - val_precision: 1.0000 - val_recall: 0.9800\n",
            "Epoch 00007: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate"
      ],
      "metadata": {
        "id": "SC4LCm1yi-IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_th = model.finetune_att_threshold(dev.text_of_post, dev.position)\n",
        "pred_offsets = model.get_toxic_offsets(test.text_of_post, threshold=opt_th)\n",
        "pred_char_offsets = model.get_toxic_char_offsets(test.text_of_post, pred_offsets)\n",
        "\n",
        "f1 = np.mean([semeval2021.f1(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "pr = np.mean([precision(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "rec = np.mean([recall(p,g) for p,g in list(zip(pred_char_offsets, test.position))])\n",
        "\n",
        "preds = model.predict(auc_eval.tokens)\n",
        "auc = roc_auc_score(auc_eval.toxicity, preds)\n",
        "\n",
        "print(\"F1: \",f1)\n",
        "print(\"Recall: \",rec)\n",
        "print(\"Precision: \" ,pr)\n",
        "print(\"ROC AUC: \",auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANWUOwPsO712",
        "outputId": "b3e33085-d19d-4955-ae38-4242adea892a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [00:01<00:00, 1826.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stopped epoch:  6\n",
            "F1:  0.4895549500454133\n",
            "Recall:  0.4895549500454133\n",
            "Precision:  0.4895549500454133\n",
            "ROC AUC:  0.8760380000000001\n"
          ]
        }
      ]
    }
  ]
}